{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Ollama Model",
  "description": "Language Model Ollama",
  "x-n8n-node-type": "lmOllama",
  "x-n8n-version": 1,
  "x-n8n-group": [
    "transform"
  ],
  "type": "object",
  "properties": {
    "model": {
      "x-title": "Model",
      "description": "The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.",
      "default": "llama3.2",
      "x-required": true,
      "type": "string"
    },
    "options": {
      "x-title": "Options",
      "description": "Additional options to add",
      "default": {},
      "type": "object",
      "properties": {
        "temperature": {
          "x-title": "Sampling Temperature",
          "description": "Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.",
          "default": 0.7,
          "type": "number",
          "minimum": 0,
          "maximum": 1,
          "x-precision": 1
        },
        "topK": {
          "x-title": "Top K",
          "description": "Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.",
          "default": -1,
          "type": "number",
          "minimum": -1,
          "maximum": 100,
          "x-precision": 1
        },
        "topP": {
          "x-title": "Top P",
          "description": "Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.",
          "default": 1,
          "type": "number",
          "minimum": 0,
          "maximum": 1,
          "x-precision": 1
        },
        "frequencyPenalty": {
          "x-title": "Frequency Penalty",
          "description": "Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition.",
          "default": 0,
          "type": "number",
          "minimum": 0
        },
        "keepAlive": {
          "x-title": "Keep Alive",
          "description": "Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes).",
          "default": "5m",
          "type": "string"
        },
        "lowVram": {
          "x-title": "Low VRAM Mode",
          "description": "Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory.",
          "default": false,
          "type": "boolean"
        },
        "mainGpu": {
          "x-title": "Main GPU ID",
          "description": "Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs.",
          "default": 0,
          "type": "number"
        },
        "numBatch": {
          "x-title": "Context Batch Size",
          "description": "Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage.",
          "default": 512,
          "type": "number"
        },
        "numCtx": {
          "x-title": "Context Length",
          "description": "The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model.",
          "default": 2048,
          "type": "number"
        },
        "numGpu": {
          "x-title": "Number of GPUs",
          "description": "Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection.",
          "default": -1,
          "type": "number"
        },
        "numPredict": {
          "x-title": "Max Tokens to Generate",
          "description": "The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs.",
          "default": -1,
          "type": "number"
        },
        "numThread": {
          "x-title": "Number of CPU Threads",
          "description": "Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection.",
          "default": 0,
          "type": "number"
        },
        "penalizeNewline": {
          "x-title": "Penalize Newlines",
          "description": "Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text",
          "default": true,
          "type": "boolean"
        },
        "presencePenalty": {
          "x-title": "Presence Penalty",
          "description": "Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity.",
          "default": 0,
          "type": "number"
        },
        "repeatPenalty": {
          "x-title": "Repetition Penalty",
          "description": "Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty.",
          "default": 1,
          "type": "number"
        },
        "useMLock": {
          "x-title": "Use Memory Locking",
          "description": "Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory.",
          "default": false,
          "type": "boolean"
        },
        "useMMap": {
          "x-title": "Use Memory Mapping",
          "description": "Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled.",
          "default": true,
          "type": "boolean"
        },
        "vocabOnly": {
          "x-title": "Load Vocabulary Only",
          "description": "Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization.",
          "default": false,
          "type": "boolean"
        },
        "format": {
          "x-title": "Output Format",
          "description": "Specifies the format of the API response",
          "default": "default",
          "type": "string",
          "enum": [
            "default",
            "json"
          ],
          "x-enumNames": [
            "Default",
            "JSON"
          ]
        }
      }
    }
  },
  "x-icon": "file:ollama.svg",
  "x-inputs": [],
  "x-outputs": [
    "ai_languageModel"
  ],
  "x-credentials": [
    {
      "name": "ollamaApi",
      "required": true
    }
  ]
}
